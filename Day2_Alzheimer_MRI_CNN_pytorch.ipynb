{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rS15ZTDQNvIc"
      },
      "source": [
        "# Alzheimer’s Disease Classification\n",
        "\n",
        "Alzheimer’s is a brain disease that affects memory and thinking. It’s the most common cause of dementia and gets worse over time. Early detection helps with better care and planning.\n",
        "\n",
        "In this session, we’ll use MRI images and a simple deep learning model (CNN) to classify brain scans into stages of Alzheimer’s.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLG_QTTVNvIf"
      },
      "source": [
        "#Dataset Overview\n",
        "\n",
        "This dataset has MRI brain scans showing different stages of Alzheimer’s disease. The images are saved in Parquet format, where each row contains one image stored as encoded bytes.\n",
        "\n",
        "- MRI Images: Brain scan data.\n",
        "- Labels: Four classes showing Alzheimer’s stage.\n",
        "- Train/Test Split: Already divided into training and testing sets.\n",
        "\n",
        "## Class Labels\n",
        "\n",
        "Each MRI scan belongs to **one of four categories**:\n",
        "\n",
        "| Label | Class Name           | Description                                |\n",
        "|-------|----------------------|--------------------------------------------|\n",
        "| **0** | Mild Demented        | Early signs of dementia.                   |\n",
        "| **1** | Moderate Demented    | Noticeable memory loss and confusion.      |\n",
        "| **2** | Non Demented         | Healthy brain, no Alzheimer’s symptoms.    |\n",
        "| **3** | Very Mild Demented   | Slight cognitive decline, minimal symptoms.|\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "reaKHC2Eg4D2"
      },
      "source": [
        "# Machine Learning workflow :\n",
        "(Recall what we did yesterday))\n",
        "\n",
        "1. Load the MRI image dataset  \n",
        "2. Preprocess the data  \n",
        "3. Build a CNN model  \n",
        "4. Train the model on training data  \n",
        "5. Evaluate on test data  \n",
        "6. Use it to make predictions\n",
        "\n",
        "We’ll go through each of these steps in the notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Load MRI Image Data"
      ],
      "metadata": {
        "id": "xTqXHYJ6fXhe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YLywbbybNvIg"
      },
      "outputs": [],
      "source": [
        "# Importing necessary libraries for machine learning and visualization\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#suppress warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REBRs6GsQPe1"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pMYP_weWW_V4"
      },
      "outputs": [],
      "source": [
        "# #Mount Google Drive\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "# train_data_file = \"/content/drive/MyDrive/SAU/Workshop BIO-ML/Lab Workbooks/Alzheimer MRI Disease Classification Dataset/Data/train-00000-of-00001-c08a401c53fe5312.parquet\"\n",
        "# test_data_file = \"/content/drive/MyDrive/SAU/Workshop BIO-ML/Lab Workbooks/Alzheimer MRI Disease Classification Dataset/Data/test-00000-of-00001-44110b9df98c5585.parquet\"\n",
        "\n",
        "# df_train = pd.read_parquet(train_data_file)\n",
        "# df_test = pd.read_parquet(test_data_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1OV6s9cGNvIh"
      },
      "outputs": [],
      "source": [
        "# We could have stored our dataset in google drive and then access it from there\n",
        "# Below code avoids the steps of authorizing access to google drive for participants\n",
        "# everytime one access the dataset\n",
        "\n",
        "\n",
        "# Alternative: google drive access (refer previous cell)\n",
        "!pip install -q gdown\n",
        "\n",
        "import gdown\n",
        "\n",
        "# file ID from the shareable link\n",
        "file_id_train = '1bk8qGUTmG6rJinUBi9viwmAbeEU9ZkPf'\n",
        "file_id_test = '1F0n85kMms-UT3OacnpeW_6LA3bl_vc2m'\n",
        "\n",
        "# Downloadable URL\n",
        "url_train = f'https://drive.google.com/uc?id={file_id_train}'\n",
        "url_test = f'https://drive.google.com/uc?id={file_id_test}'\n",
        "\n",
        "# Download the file to the current working directory\n",
        "gdown.download(url_train, 'train.parquet', quiet=False)\n",
        "gdown.download(url_test, 'test.parquet', quiet=False)\n",
        "\n",
        "\n",
        "df_train = pd.read_parquet('train.parquet')\n",
        "df_test = pd.read_parquet('test.parquet')\n",
        "\n",
        "# Notice that the data is stored in 'parquet' file format\n",
        "# It's commonly used for storing large dataset where the data is stored column-wise\n",
        "# instead of row-wise. Allows high compression and fast reading."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U1s6OnbxNvIh"
      },
      "outputs": [],
      "source": [
        "#Add .head(), .info() or class distribution plot\n",
        "df_train.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bvy3r8b-NvIi"
      },
      "source": [
        "#Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DOzq-bBHNvIj"
      },
      "source": [
        "The MRI scans in Parquet format store images as byte strings inside a ditionary . To use them for deep learning, we convert these bytes into grayscale image arrays using np.frombuffer and cv2.imdecode."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d8gM9UGONvIj"
      },
      "outputs": [],
      "source": [
        "# cv2 library is part of OpenCV(Open Source computer vision library) used for\n",
        "# preprocessing images before feeding them to machine learning models\n",
        "\n",
        "# MRI scans are typically grayscale; using IMREAD_GRAYSCALE ensures consistent shape and format\n",
        "import cv2\n",
        "def dict_to_image(image_dict):\n",
        "    if isinstance(image_dict, dict) and 'bytes' in image_dict:\n",
        "        byte_string = image_dict['bytes']\n",
        "        nparr = np.frombuffer(byte_string, np.uint8)\n",
        "        img = cv2.imdecode(nparr, cv2.IMREAD_GRAYSCALE)\n",
        "        return img\n",
        "    else:\n",
        "        raise TypeError(f\"Expected dictionary with 'bytes' key, got {type(image_dict)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DfsMEPkFNvIj"
      },
      "source": [
        "Apply above function to transform all images in the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GJ9Nz8CbNvIl"
      },
      "outputs": [],
      "source": [
        "df_train['img_arr'] = df_train['image'].apply(dict_to_image)\n",
        "df_train.drop(\"image\", axis=1, inplace=True)\n",
        "df_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gSlDxSdjNvIm"
      },
      "outputs": [],
      "source": [
        "# Create a dictionary to map labels to class names\n",
        "label_mapping = {\n",
        "    0: \"Mild_Demented\",\n",
        "    1: \"Moderate_Demented\",\n",
        "    2: \"Non_Demented\",\n",
        "    3: \"Very_Mild_Demented\"\n",
        "}\n",
        "\n",
        "df_train['class_name'] = df_train['label'].map(label_mapping)\n",
        "\n",
        "#df_train.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qD4ZhsXVNvIn"
      },
      "source": [
        "#Explore the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "knuF2qtVNvIn"
      },
      "outputs": [],
      "source": [
        "#Plot some of the MRI images\n",
        "fig, ax = plt.subplots(2, 3, figsize=(15, 5))\n",
        "axs = ax.flatten()\n",
        "for axes in axs:\n",
        "    rand = np.random.randint(0, len(df_train))\n",
        "    axes.imshow(df_train.iloc[rand]['img_arr'], cmap=\"gray\")\n",
        "    axes.set_title([df_train.iloc[rand]['class_name']])\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eUrRrBTBNvIn"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "\n",
        "# Countplot to visualize the distribution of classes\n",
        "sns.countplot(data=df_train, x='class_name', palette=\"viridis\")\n",
        "\n",
        "# Add labels and title\n",
        "plt.xlabel(\"Alzheimer Category\")\n",
        "plt.ylabel(\"Number of Images\")\n",
        "plt.title(\"Distribution of Alzheimer MRI Categories\")\n",
        "plt.xticks(rotation=20)\n",
        "\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l5vIEp8KNvIo"
      },
      "outputs": [],
      "source": [
        "df_train['class_name'].value_counts()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ynbth88oNvIr"
      },
      "source": [
        "*Imbalanced dataset as non_demented and ver_mild_demented dominate the dataset while moderate_demented is very low*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Train-Test Split"
      ],
      "metadata": {
        "id": "haJLLrlbfmnK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W08Rv7m6NvIt"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_df, val_df = train_test_split(df_train, test_size=0.2, stratify=df_train['class_name'], random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OPOC1Dn3NvIu"
      },
      "outputs": [],
      "source": [
        "print(train_df.shape,val_df.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5yyOkg0NvIv"
      },
      "source": [
        "* Normalize images:\n",
        "When working with image data, pixel values typically range from 0 to 255\n",
        "(since images are stored as 8-bit integers). Normalizing them by dividing by 255 scales these values between 0 and 1.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bZifHmfJNvIv"
      },
      "outputs": [],
      "source": [
        "#image pixel values (0, 255) -> (0, 1)\n",
        "train_df['img_arr'] = train_df['img_arr'].apply(lambda x: x / 255.0)\n",
        "val_df['img_arr'] = val_df['img_arr'].apply(lambda x: x / 255.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4VGJlvSMNvIv"
      },
      "outputs": [],
      "source": [
        "print(\"Training Set Class Distribution:\\n\", train_df['class_name'].value_counts())\n",
        "print(\"\\nValidation Set Class Distribution:\\n\", val_df['class_name'].value_counts())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4Yw30E8NvIv"
      },
      "source": [
        "#Model Building/Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dv4ORP1dNvIw"
      },
      "source": [
        "CNN: Convolution, Pooling, Dense/Fully Connected layers\n",
        "\n",
        "Convolution and Max/Average pooling: https://colab.research.google.com/drive/1EsBZi0Y-pu47BZRNI2JlG3Cpyb9RoSyY?usp=sharing"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RZ6RJ-92rrHf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zI61XKuPOrS9"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch.optim as optim\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "# Check device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# === Prepare Train and Validation Data ===\n",
        "X_train = np.stack(train_df['img_arr'].values).reshape(-1, 1, 128, 128).astype(np.float32)\n",
        "y_train = train_df['label'].values.astype(np.int64)\n",
        "\n",
        "X_val = np.stack(val_df['img_arr'].values).reshape(-1, 1, 128, 128).astype(np.float32)\n",
        "y_val = val_df['label'].values.astype(np.int64)\n",
        "\n",
        "train_dataset = TensorDataset(torch.tensor(X_train), torch.tensor(y_train))\n",
        "val_dataset = TensorDataset(torch.tensor(X_val), torch.tensor(y_val))\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32)\n",
        "\n",
        "# CNN Model\n",
        "class CNNModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNNModel, self).__init__()\n",
        "        # Convolutional layer: detects patterns in image patches\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3)\n",
        "        # Pooling layer: reduces spatial dimensions while retaining important features\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n",
        "        # Fully connected (dense) layer: combines features for final prediction\n",
        "        self.fc1 = nn.Linear(64 * 30 * 30, 128)\n",
        "        self.fc2 = nn.Linear(128, 4)  # 4 classes\n",
        "\n",
        "\n",
        "    # Forward method defines how input flows through the network\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))   #\n",
        "        x = self.pool(F.relu(self.conv2(x)))   #\n",
        "\n",
        "        # Flatten the 2D features into 1D before passing to dense layers\n",
        "        x = x.view(-1, 64 * 30 * 30) #old\n",
        "\n",
        "        x = F.relu(self.fc1(x))\n",
        "\n",
        "        return self.fc2(x)\n",
        "\n",
        "model = CNNModel().to(device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Architecture\n",
        "Uncomment the code in cell below to print the Model Architecture. There will be some additional libraries installed so it may take a while!"
      ],
      "metadata": {
        "id": "26PLjenDPrAS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Po6K1nAHp95b"
      },
      "outputs": [],
      "source": [
        "    # Model Architecture\n",
        "    # !pip install torchviz\n",
        "    # import torch\n",
        "    # from torchviz import make_dot\n",
        "    # import matplotlib.pyplot as plt\n",
        "\n",
        "    # def plot_model(model, input_data, filename=\"model_architecture.png\"):\n",
        "    #     \"\"\"Plots the model architecture.\"\"\"\n",
        "    #     dot = make_dot(model(input_data), params=dict(model.named_parameters()))\n",
        "    #     dot.format = 'png'\n",
        "    #     dot.render(filename)\n",
        "\n",
        "    # # Example Usage\n",
        "    # model1 = CNNModel()  # CNN model\n",
        "    # input_data = torch.randn(1, 1, 128, 128) #  input data\n",
        "    # plot_model(model1, input_data)\n",
        "    # plot_model(model1, input_data, filename=\"model_architecture.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a_YJ-43WOvgv"
      },
      "outputs": [],
      "source": [
        "# Define loss function to measure prediction error\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# Define optimizer to adjust model weights based on loss\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training with Early Stopping\n",
        "num_epochs = 20 #10 #50\n",
        "patience = 5\n",
        "best_val_loss = np.inf\n",
        "early_stop_counter = 0\n",
        "\n",
        "train_losses, val_losses = [], []\n",
        "\n",
        "# epochs: An epoch refers to one complete pass through the entire training dataset\n",
        "# by the model. If you have 1000 images and train for 10 epochs, Your model will see\n",
        "# all 1000 images 10 times (in different orders, usually). Imagine studying for an\n",
        "# exam. You don't read your notes just once—you review them several times to really\n",
        "# understand and retain the information. That’s what epochs do for a model!\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # Training mode: enable gradient updates\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    batch_count = 0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "        batch_count += 1\n",
        "        if batch_count % 16 == 0:\n",
        "            print(f\"Epoch: {epoch+1}, iteration: {batch_count}/128, loss : {loss.item()}\")\n",
        "\n",
        "    train_loss = running_loss / len(train_loader)\n",
        "    train_losses.append(train_loss)\n",
        "\n",
        "    # Evaluation mode: disable gradient updates\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    # Disable gradient calculation for validation/inference\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    val_loss /= len(val_loader)\n",
        "    val_losses.append(val_loss)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}\")\n",
        "\n",
        "    # Early Stopping Check\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        torch.save(model.state_dict(), 'best_model.pth')\n",
        "        early_stop_counter = 0\n",
        "    else:\n",
        "        early_stop_counter += 1\n",
        "        if early_stop_counter >= patience:\n",
        "            print(\"Early stopping triggered.\")\n",
        "            break\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Summary of Model Parameters"
      ],
      "metadata": {
        "id": "K9cJjY2Zgmrf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sTCGu85apwDG"
      },
      "outputs": [],
      "source": [
        "!pip install torchinfo\n",
        "from torchinfo import summary\n",
        "summary(model, input_size=(1, 1, 128, 128))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BDqJ_qhYO0IX"
      },
      "outputs": [],
      "source": [
        "# Load best model\n",
        "model.load_state_dict(torch.load('best_model.pth'))\n",
        "\n",
        "# Evaluate on Validation Set\n",
        "model.eval()\n",
        "all_preds = []\n",
        "with torch.no_grad():\n",
        "    for images, _ in val_loader:\n",
        "        images = images.to(device)\n",
        "        outputs = model(images)\n",
        "        preds = torch.argmax(outputs, dim=1)\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "\n",
        "print(\"Validation Classification Report:\\n\", classification_report(y_val, all_preds))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "myJVpbs3O3-H"
      },
      "outputs": [],
      "source": [
        "# Confusion Matrix\n",
        "plt.figure(figsize=(6,6))\n",
        "sns.heatmap(confusion_matrix(y_val, all_preds), annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['Mild', 'Moderate', 'Non-Demented', 'Very Mild'],\n",
        "            yticklabels=['Mild', 'Moderate', 'Non-Demented', 'Very Mild'])\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Validation Confusion Matrix')\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TujGLHwQO670"
      },
      "outputs": [],
      "source": [
        "# Plot Training vs. Validation Loss\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(range(1, len(train_losses)+1), train_losses, 'bo-', label='Training Loss')\n",
        "plt.plot(range(1, len(val_losses)+1), val_losses, 'r^-', label='Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training vs. Validation Loss')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Prediction"
      ],
      "metadata": {
        "id": "JvMPgY8Aj644"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "guidy3JSj6vV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AzLchvEiO97U"
      },
      "outputs": [],
      "source": [
        "# Prediction on Test Data\n",
        "df_test['img_arr'] = df_test['image'].apply(dict_to_image)\n",
        "df_test.drop(\"image\", axis=1, inplace=True)\n",
        "\n",
        "X_test = np.stack(df_test['img_arr'].values).reshape(-1, 1, 128, 128).astype(np.float32) / 255.0\n",
        "y_test = df_test['label'].values.astype(np.int64)\n",
        "\n",
        "test_dataset = TensorDataset(torch.tensor(X_test), torch.tensor(y_test))\n",
        "test_loader = DataLoader(test_dataset, batch_size=32)\n",
        "\n",
        "model.eval()\n",
        "all_test_preds = []\n",
        "with torch.no_grad():\n",
        "    for images, _ in test_loader:\n",
        "        images = images.to(device)\n",
        "        outputs = model(images)\n",
        "        preds = torch.argmax(outputs, dim=1)\n",
        "        all_test_preds.extend(preds.cpu().numpy())\n",
        "\n",
        "accuracy = accuracy_score(y_test, all_test_preds)\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "print(\"Test Classification Report:\\n\", classification_report(y_test, all_test_preds))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Grad-CAM Visualization\n",
        "\n",
        "Grad-CAM produces a heatmap overlay on the original input image, highlighting which parts of the image the CNN focused on when making its prediction.\n",
        "\n",
        "- Red/Hot regions: The model found these parts most important for its\n",
        "decision.\n",
        "- Green = Medium importance. The region contributed to the decision, but not as strongly.\n",
        "- Cool/Blue regions: These were less relevant.\n",
        "\n",
        "\n",
        "Question:\n",
        "- Does the focus area in Grad-CAM overlay make clinical sense?\n",
        "- Is the model consistent across similar predictions?\n",
        "- Trust in AI: Would a doctor feel confident with this kind of explanation?\n",
        "- Note: Try multiple examples to check if the model is focusing on medically relevant regions consistently."
      ],
      "metadata": {
        "id": "lmzkoxtC8loT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GradCAM:\n",
        "    def __init__(self, model, target_layer):\n",
        "        self.model = model\n",
        "        self.target_layer = target_layer\n",
        "        self.gradients = None\n",
        "        self.activations = None\n",
        "\n",
        "        # Register hooks\n",
        "        self.target_layer.register_forward_hook(self.save_activation)\n",
        "        self.target_layer.register_backward_hook(self.save_gradient)\n",
        "\n",
        "    def save_activation(self, module, input, output):\n",
        "        self.activations = output.detach()\n",
        "\n",
        "    def save_gradient(self, module, grad_input, grad_output):\n",
        "        self.gradients = grad_output[0].detach()\n",
        "\n",
        "    def generate(self, input_tensor, class_idx=None):\n",
        "        output = self.model(input_tensor)\n",
        "\n",
        "        if class_idx is None:\n",
        "            class_idx = output.argmax().item()\n",
        "\n",
        "        self.model.zero_grad()\n",
        "        loss = output[0, class_idx]\n",
        "        loss.backward()\n",
        "\n",
        "        weights = self.gradients.mean(dim=[2, 3], keepdim=True)\n",
        "        cam = (weights * self.activations).sum(dim=1, keepdim=True)\n",
        "\n",
        "        cam = F.relu(cam)\n",
        "        cam = F.interpolate(cam, size=input_tensor.shape[2:], mode='bilinear', align_corners=False)\n",
        "        cam = cam.squeeze().cpu().numpy()\n",
        "\n",
        "        cam = (cam - cam.min()) / (cam.max() - cam.min() + 1e-8)  # normalize\n",
        "        return cam\n"
      ],
      "metadata": {
        "id": "pY3Qabvb2JKn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_gradcam_on_image(img_tensor, cam, alpha=0.4):\n",
        "    img = img_tensor.squeeze().cpu().numpy()\n",
        "    if img.ndim == 2:\n",
        "        img = np.stack([img]*3, axis=-1)\n",
        "\n",
        "    cam_resized = cv2.resize(cam, (img.shape[1], img.shape[0]))\n",
        "    heatmap = cv2.applyColorMap(np.uint8(255 * cam_resized), cv2.COLORMAP_JET)\n",
        "    heatmap = np.float32(heatmap) / 255\n",
        "\n",
        "    overlay = heatmap * alpha + img / img.max()\n",
        "    overlay = overlay / overlay.max()\n",
        "\n",
        "    plt.imshow(overlay)\n",
        "    plt.axis('off')\n",
        "    plt.title(\"Grad-CAM Overlay\")\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "5KVxCkve8oCX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load an image tensor\n",
        "rand = np.random.randint(0, len(df_train))\n",
        "sample_img = df_train.iloc[rand]['img_arr']\n",
        "input_tensor = torch.tensor(sample_img, dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(device) / 255.0\n",
        "\n",
        "# Apply Grad-CAM to conv2\n",
        "gradcam = GradCAM(model, model.conv2)\n",
        "cam = gradcam.generate(input_tensor)\n",
        "\n",
        "# Visualize\n",
        "show_gradcam_on_image(input_tensor.cpu(), cam)\n"
      ],
      "metadata": {
        "id": "NQyLu6jD8q1E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Quiz"
      ],
      "metadata": {
        "id": "ieU8zbmzjxGM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Quiz: MRI and CNN Understanding\n",
        "\n",
        "# Question 1\n",
        "ans = input(\"1. Why do we use grayscale images instead of color for brain MRI scans?\\n(a) Grayscale images are more artistic\\n(b) Brain MRIs are naturally in grayscale and color adds noise\\n(c) Grayscale saves storage but reduces accuracy\\n(d) CNNs cannot work with color images\\nYour answer: \")\n",
        "if ans.lower() == 'b':\n",
        "    print(\"Correct! Medical imaging like MRI is naturally in grayscale to emphasize tissue contrast.\")\n",
        "else:\n",
        "    print(\"Incorrect. The correct answer is (b).\")\n",
        "\n",
        "# Question 2\n",
        "ans = input(\"\\n2. What does a Convolutional Neural Network (CNN) learn from an image?\\n(a) Exact pixel values\\n(b) Patterns and features like edges, shapes, and textures\\n(c) Text labels on the image\\n(d) The file name of the image\\nYour answer: \")\n",
        "if ans.lower() == 'b':\n",
        "    print(\"Correct! CNNs learn spatial features like edges and textures that help in classification.\")\n",
        "else:\n",
        "    print(\"Incorrect. The correct answer is (b).\")\n",
        "\n",
        "# Question 3\n",
        "ans = input(\"\\n3. What does a Conv2D layer do in a CNN model?\\n(a) Adds text annotations to images\\n(b) Converts images to 1D arrays\\n(c) Applies filters to detect features like edges and patterns\\n(d) Shrinks the image size\\nYour answer: \")\n",
        "if ans.lower() == 'c':\n",
        "    print(\"Correct! Conv2D applies filters to extract features like edges and textures from images.\")\n",
        "else:\n",
        "    print(\"Incorrect. The correct answer is (c).\")\n",
        "\n",
        "# Question 4\n",
        "ans = input(\"\\n4. What is the purpose of a MaxPooling2D layer in CNNs?\\n(a) To shuffle the image pixels\\n(b) To reduce the size of the feature maps and keep only the most important information\\n(c) To make the image larger\\n(d) To increase model accuracy directly\\nYour answer: \")\n",
        "if ans.lower() == 'b':\n",
        "    print(\"Correct! MaxPooling2D helps reduce the image size while retaining important features.\")\n",
        "else:\n",
        "    print(\"Incorrect. The correct answer is (b).\")\n",
        "\n",
        "# Question 5\n",
        "ans = input(\"\\n5. Why is it important to resize all MRI images to the same shape (e.g., 128x128)?\\n(a) So they look nicer\\n(b) Because models only accept images of equal shape\\n(c) To reduce brightness\\n(d) To improve color accuracy\\nYour answer: \")\n",
        "if ans.lower() == 'b':\n",
        "    print(\"Correct! Neural networks need consistent input dimensions.\")\n",
        "else:\n",
        "    print(\"Incorrect. The correct answer is (b).\")\n",
        "\n",
        "# Question 6\n",
        "ans = input(\"\\n6. What is the role of the final layer (softmax) in our CNN model?\\n(a) To detect edges\\n(b) To convert features into class probabilities\\n(c) To shuffle the data\\n(d) To normalize pixel values\\nYour answer: \")\n",
        "if ans.lower() == 'b':\n",
        "    print(\"Correct! Softmax turns outputs into probabilities for each class.\")\n",
        "else:\n",
        "    print(\"Incorrect. The correct answer is (b).\")\n",
        "\n",
        "# Question 7\n",
        "ans = input(\"\\n7. Why is it useful to use MRI scans for detecting Alzheimer's disease?\\n(a) MRIs are colorful\\n(b) They show brain structure changes that relate to disease\\n(c) Doctors prefer images over tests\\n(d) They contain the patient’s DNA\\nYour answer: \")\n",
        "if ans.lower() == 'b':\n",
        "    print(\"Correct! MRI scans reveal structural changes in the brain linked to neurodegeneration.\")\n",
        "else:\n",
        "    print(\"Incorrect. The correct answer is (b).\")\n",
        "\n",
        "# Question 8\n",
        "ans = input(\"\\n8. What does it mean when we say a model is 'trained'?\\n(a) It has learned to make predictions based on patterns in data\\n(b) It memorized the patient names\\n(c) It runs faster\\n(d) It creates new diseases\\nYour answer: \")\n",
        "if ans.lower() == 'a':\n",
        "    print(\"Correct! Training is the process of learning from examples to make predictions.\")\n",
        "else:\n",
        "    print(\"Incorrect. The correct answer is (a).\")\n",
        "\n",
        "# Question about epochs\n",
        "ans = input(\"\\n9. What is an 'epoch' in machine learning?\\n(a) A single pass through the entire training dataset\\n(b) A type of neural network\\n(c) A mistake in the data\\n(d) A hardware device used for training\\nYour answer: \")\n",
        "if ans.lower() == 'a':\n",
        "    print(\"Correct! An epoch is one full pass through all the training data.\")\n",
        "else:\n",
        "    print(\"Incorrect. The correct answer is (a). An epoch means the model has seen all training samples once.\")\n",
        "\n",
        "\n",
        "# Question about multiple epochs\n",
        "ans = input(\"\\n10. Why do we train a model for multiple epochs?\\n(a) To help the model learn better by refining its predictions over time\\n(b) To waste computer resources\\n(c) To make the dataset larger\\n(d) To create more labels\\nYour answer: \")\n",
        "if ans.lower() == 'a':\n",
        "    print(\"Correct! Training for multiple epochs allows the model to gradually improve and reduce errors.\")\n",
        "else:\n",
        "    print(\"Incorrect. The correct answer is (a). Multiple epochs help the model learn from the data more effectively.\")\n",
        "\n",
        "# Question: Why use Early Stopping?\n",
        "ans = input(\"\\n11. Why do we use 'early stopping' during model training?\\n\"\n",
        "            \"(a) To save electricity\\n\"\n",
        "            \"(b) To avoid overfitting by stopping when validation loss stops improving\\n\"\n",
        "            \"(c) To make training easier to watch\\n\"\n",
        "            \"(d) To restart the training from scratch\\n\"\n",
        "            \"Your answer: \")\n",
        "\n",
        "if ans.lower() == 'b':\n",
        "    print(\"Correct! Early stopping helps prevent overfitting by stopping training when the model stops improving on validation data.\")\n",
        "else:\n",
        "    print(\"Incorrect. The correct answer is (b). Early stopping monitors performance on validation data and stops training to avoid overfitting.\")\n",
        "\n",
        "\n",
        "# Question: MRI vs fMRI\n",
        "# ans = input(\"\\n12. What is a key difference between MRI and fMRI scans?\\n\"\n",
        "#             \"(a) MRI captures brain activity while fMRI captures brain structure\\n\"\n",
        "#             \"(b) MRI uses colors to show brain functions\\n\"\n",
        "#             \"(c) fMRI captures changes in brain activity over time\\n\"\n",
        "#             \"(d) fMRI is only used in animals\\n\"\n",
        "#             \"Your answer: \")\n",
        "\n",
        "# if ans.lower() == 'c':\n",
        "#     print(\"Correct! fMRI tracks brain activity by measuring changes in blood flow over time.\")\n",
        "# else:\n",
        "#     print(\"Incorrect. The correct answer is (c). Unlike MRI, which gives static anatomical images, fMRI captures brain activity over time.\")"
      ],
      "metadata": {
        "id": "KwArcsn_9dtC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Common Alzheimer’s Terminology\n",
        "\n",
        "| Abbreviation | Full Form                          | Meaning / Use |\n",
        "|--------------|-------------------------------------|----------------|\n",
        "| **AD**       | Alzheimer’s Disease                 | The most common type of dementia, causing memory loss and cognitive decline. |\n",
        "| **MCI**      | Mild Cognitive Impairment           | A stage between normal aging and dementia; noticeable decline but not severely disabling. |\n",
        "| **CN**       | Cognitively Normal                  | Individuals with normal cognitive functioning; used as control in studies. |\n",
        "| **MRI**      | Magnetic Resonance Imaging          | Brain scan to detect structural changes in the brain. |\n",
        "| **fMRI**     | Functional MRI                      | Measures brain activity through blood flow changes. |\n",
        "| **CSF**      | Cerebrospinal Fluid                 | Fluid around brain/spine; tested for Alzheimer’s biomarkers. |\n",
        "| **Aβ (A-beta)** | Amyloid Beta                     | Protein that forms brain plaques in Alzheimer’s disease. |\n",
        "| **Tau**      | Tau Protein                         | Protein that forms tangles inside neurons in Alzheimer’s. |\n",
        "| **PET**      | Positron Emission Tomography        | Imaging that visualizes amyloid and tau buildup. |\n",
        "| **NIA-AA**   | National Institute on Aging – Alzheimer’s Association | Sets research and diagnostic guidelines for Alzheimer’s. |\n",
        "\n"
      ],
      "metadata": {
        "id": "84I8zhtoel1F"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QuMjIJCTzgBB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Difference in AI Model Design: MRI vs fMRI\n",
        "\n",
        "| **Aspect**             | **MRI (Structural MRI)**                                             | **fMRI (Functional MRI)**                                                             |\n",
        "|------------------------|----------------------------------------------------------------------|----------------------------------------------------------------------------------------|\n",
        "| **Type of Data**       | Static 3D anatomical image (e.g., brain structure)                   | 4D time-series data (3D images over time) capturing brain activity                    |\n",
        "| **Typical Use**        | Detecting structural changes (e.g., atrophy in Alzheimer’s)          | Studying brain function, networks, and responses to tasks/stimuli                    |\n",
        "| **Input to Model**     | Single 2D or 3D grayscale image                                       | Sequence of 3D images over time (like a video)                                       |\n",
        "| **Preprocessing**      | Image resizing, normalization, skull-stripping                      | All MRI steps **plus** temporal filtering, motion correction, time-series alignment  |\n",
        "| **Model Type**         | 2D or 3D CNN                                                         | 3D CNN, RNN (like LSTM), or hybrid (CNN + RNN)                                       |\n",
        "| **Data Size**          | Smaller, easier to train on modest hardware                          | Larger, requires more memory and often downsampling                                  |\n",
        "| **Example Architecture**| 2D CNN for image classification                                     | CNN + LSTM for activity pattern recognition over time                                |\n",
        "| **Applications**       | Disease staging, tumor detection, brain structure classification     | Brain region connectivity analysis, mental task decoding, emotion detection          |\n",
        "\n",
        "---\n",
        "\n",
        "### Simplified Analogy\n",
        "\n",
        "- **MRI** is like a **photo** of the brain.\n",
        "- **fMRI** is like a **video** showing how the brain is working over time.\n",
        "\n",
        "---\n",
        "\n",
        "### Summary\n",
        "\n",
        "| MRI:  | Focused on **structure** → best with **CNNs (2D or 3D)**.        |\n",
        "|-------|------------------------------------------------------------------|\n",
        "| fMRI: | Focused on **brain function** → needs models that handle time, like **CNN + RNN**. |\n"
      ],
      "metadata": {
        "id": "TMufbgECet9A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ySqvZe5reyuV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Notes\n",
        "- Add google drive access code to let the user know, how to access the dataset.\n",
        "and why did I use gdown(to allow simpler access)\n",
        "\n",
        "- Simplify the code and explanation so that participants can get something out of it and have motivation to explore on their own.\n",
        "\n",
        "- Some image for pooling apart from convolution. We can discuss kernels/filter convolution and pooling which does shrinking and how the two form the core of CNN for feature engineering. And finally fully connected layers to move towards prediction.\n",
        "\n",
        "- If possible, convey relevance/importance of Convolution allowing sparse connection and weight sharing which reduces the number of parameter in the network greatly.\n",
        "\n",
        "- Also discuss how for identifying a shape for example a car, identifying edges is helpful (we don't care about car color etc). Also how in the past sift/hog kernel were used for feature engineering and then ML tech applied but now with CNN we try to learn the parameters of those filters by ourself.\n",
        "\n",
        "- Instead of handcrafted features in the past, we try to learn the parameters of the filters in CNN.\n",
        "\n",
        "- And not just one filter, but multiple layer of filters, in addition to learning the weights of the classifier (using backpropagation).\n",
        "\n",
        "- Multiple learnt convolutions at every layer and multiple such layers.\n",
        "\n",
        "- Discuss if relevant, why CNN why not Feed Forward. convey how cnn capture and benefit from the structure of the image (interaction between nearby or neighboring pixels is more interesting). This leads to sparse connectivity and reduces the number of parameters in the network.\n",
        "\n",
        "- Further we try to learn more and more abstract representation at each layer (where at some point, they don't make much sense to naked eye, but contains information for Neural network and predictions/inferences.\n",
        "\n",
        "- Add more discussion points everywhere, specially in introductory workbook.\n",
        "\n",
        "- Pytorch CNN model, forward, conv2d etc\n",
        "- Reference Notebook: https://www.kaggle.com/code/aasthadata/alzheimer-mri-cnn-0-96 (in Tensorflow). Share this link if someone wants to use tensorflow.\n",
        "\n",
        "- Use GPUs for training, should be faster. Otherwise even going through 1-2 epoch during demo time would be difficult.\n"
      ],
      "metadata": {
        "id": "mIhIsA1Ve90Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#End"
      ],
      "metadata": {
        "id": "G8nXj2oLeubk"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "datasetId": 5152637,
          "sourceId": 8610355,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30919,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}